/-
Copyright (c) 2024 Chenyi Li. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Chenyi Li,
-/
import Mathlib.Analysis.Calculus.Gradient.Basic
import Mathlib.Analysis.Calculus.ContDiff.Defs
import Mathlib.Analysis.InnerProductSpace.Positive
import Function.Convex_Function
import Optimality.Optimality_Condition_of_Unconstrained_Problem

/-!
# Unconstrained_Problem

## Main results

This file contains the following parts of unconstrained optimization problem.
* the definition of an unconstrained optimization prblem
* the definition of a local minima, global minima, strict local minima, stationary point
* the first order necessary condition of unconstrained optimization problem
* the second order necessary and sufficient condition of unconstrained optimization problem
* the convexity of a function implies local minima is global minima
-/

section additional_definitions

variable {E : Type _} [NormedAddCommGroup E] [CompleteSpace E] [IsROrC ùïú] [InnerProductSpace ùïú E]

/-
  the definition of a self-adjoint operator which is strict positive.
-/
def ContinuousLinearMap.IsStrictPositive (T : E ‚ÜíL[ùïú] E) : Prop :=
  IsSelfAdjoint T ‚àß ‚àÄ x ‚â† 0, 0 < T.reApplyInnerSelf x

end additional_definitions

section optimization

variable {E : Type _} [NormedAddCommGroup E] [CompleteSpace E] [InnerProductSpace ‚Ñù E]

open Set

/-
  the definition of an unconstrained optimization problem.
  The objective function is a function from a Hilbert space to ‚Ñù.
-/
structure Unconstrained_OptimizationProblem (E : Type _) :=
(objective : E ‚Üí ‚Ñù)

namespace Unconstrained_OptimizationProblem

open Unconstrained_OptimizationProblem

variable (p : Unconstrained_OptimizationProblem E)

/-
  A point x‚ÇÅ is a global minimizer if f x‚ÇÅ ‚â§ f x for all x
-/
def Global_Minima (point : E) : Prop :=
  IsMinOn p.objective univ point

/-
  A point x‚ÇÅ is a local minimizer if there is a neighborhood B of x‚ÇÅ
  such that f x‚ÇÅ ‚â§ f x for all x ‚àà B .
-/
def Local_Minima (point : E) : Prop :=
  IsLocalMinOn p.objective univ point

/-
  A point x‚ÇÅ is a strict local minimizer (also called a strong local minimizer) if there is a
  neighborhood B of x‚àó such that f x‚ÇÅ < f x for all x ‚àà N with x ‚â† x‚ÇÅ.
-/
def Strict_Local_Minima (point : E) : Prop :=
  ‚àÉ Œµ > 0, ‚àÄ y, y ‚àà Metric.ball point Œµ ‚Üí y ‚â† point ‚Üí p.objective point > p.objective y

/-
  x is a stationary point if f i differentiable and ‚àá f (x) = 0
-/
def Stationary_Point (point : E) : Prop :=
  Differentiable ‚Ñù p.objective ‚àß gradient p.objective point = (0 : E)

/-
  A point x‚ÇÅ is a global maximizer if f x‚ÇÅ ‚â• f x for all x
-/
def Global_Maxima (point : E) : Prop :=
  IsMaxOn p.objective univ point

/-
  A point x‚ÇÅ is a local maximizer if there is a neighborhood B of x‚ÇÅ
  such that f x‚ÇÅ ‚â• f x for all x ‚àà B .
-/
def Local_Maxima (point : E) : Prop :=
  IsLocalMaxOn p.objective univ point

end Unconstrained_OptimizationProblem

section First_Order_Optimality

open Topology InnerProductSpace Set Filter Tendsto

variable (p : Unconstrained_OptimizationProblem E) (loc : E)

/- any global minima must be local minima -/
lemma global_minima_is_local_minima
    (h : p.Global_Minima loc) : p.Local_Minima loc := by
  rw [Unconstrained_OptimizationProblem.Local_Minima]
  rw [Unconstrained_OptimizationProblem.Global_Minima] at h
  exact IsMinOn.localize h

/-
If the function f is first-order continuously differentiable, then
the gradient of f is continuous.
-/
lemma gradient_continuous_of_contdiff {x : E} {Œµ : ‚Ñù} (f : E ‚Üí ‚Ñù)
    (he : Œµ > 0) (hf : ContDiffOn ‚Ñù 1 f (Metric.ball x Œµ)) :
    ContinuousAt (fun x ‚Ü¶ gradient f x) x := by
  let s := Metric.ball x Œµ
  have h : ContDiffAt ‚Ñù 1 f x := by
    apply ContDiffOn.contDiffAt hf
    rw [mem_nhds_iff]; use s
    exact ‚ü®Eq.subset rfl, ‚ü®Metric.isOpen_ball, Metric.mem_ball_self he‚ü©‚ü©
  rw [contDiffAt_one_iff] at h
  rcases h with ‚ü®f', ‚ü®u, ‚ü®hu1, ‚ü®hu2, hu3‚ü©‚ü©‚ü©‚ü©
  have : Set.EqOn (fun y ‚Ü¶ (LinearIsometryEquiv.symm (toDual ‚Ñù E)) (f' y))
      (fun y ‚Ü¶ gradient f y) u := by
    intro z hz; dsimp
    specialize hu3 z hz
    rw [hasFDerivAt_iff_hasGradientAt] at hu3
    have : HasGradientAt f (gradient f z) z :=
      DifferentiableAt.hasGradientAt hu3.differentiableAt
    exact HasGradientAt.unique hu3 this
  have hcon1 : ContinuousOn (fun y ‚Ü¶ (LinearIsometryEquiv.symm (toDual ‚Ñù E)) (f' y)) u :=
    Continuous.comp_continuousOn (LinearIsometryEquiv.continuous _) hu2
  rw [(continuousOn_congr this)] at hcon1
  apply ContinuousOn.continuousAt hcon1 hu1

/-
If x‚àó is a local minimizer and f is continuously differentiable in an open neighborhood
of x‚àó, then ‚àá f (x‚àó) = 0 .
Numerical Optimization, Nocedal & Wright, Theorem 2.2
-/
theorem first_order_unconstrained_necessary
    (h : ‚àÉ Œµ > 0, ContDiffOn ‚Ñù 1 p.objective (Metric.ball loc Œµ))
    (hl : p.Local_Minima loc) :
    gradient p.objective loc = (0 : E) := by
  by_contra hn; push_neg at hn
  rw [Unconstrained_OptimizationProblem.Local_Minima] at hl
  rw [IsLocalMinOn, IsMinFilter, eventually_iff_exists_mem] at hl
  rcases hl with ‚ü®v, ‚ü®hv, hv2‚ü©‚ü©
  rw [nhdsWithin] at hv; simp at hv;
  have hy : ‚àÉ s > 0, (Metric.ball loc s) ‚äÜ v := by
    exact Metric.mem_nhds_iff.mp hv
  rcases hy with ‚ü®s, ‚ü®hs, hy‚ü©‚ü©
  rcases h with ‚ü®Œµ, ‚ü®he, hd‚ü©‚ü©
  let rad := min s Œµ
  have pos : 0 < rad := by simp [hs, he]
  have hd : ContDiffOn ‚Ñù 1 p.objective (Metric.ball loc rad) :=
    hd.mono (Metric.ball_subset_ball (min_le_right s Œµ))
  have hy : Metric.ball loc rad ‚äÜ v :=
    Set.Subset.trans (Metric.ball_subset_ball (min_le_left s Œµ)) hy
  let d := (- gradient p.objective loc)
  let g : ‚Ñù ‚Üí ‚Ñù := fun t ‚Ü¶ inner d (gradient p.objective (loc + t ‚Ä¢ d))
  have hg : g 0 < 0 := by
    simp; by_contra hh; push_neg at hh;
    rw [real_inner_self_nonpos] at hh; exact hn hh
  let rad1 := rad / (2 * ‚Äñd‚Äñ)
  have t1 : ‚Äñd‚Äñ ‚â† 0 := by simp [hn]
  have t2 : d ‚â† 0 := by simp [hn]
  have pos1 : 0 < rad1 := by
    apply div_pos pos
    apply mul_pos; simp; exact norm_pos_iff.mpr t2
  have hin : ‚àÄ t ‚àà Icc 0 rad1, (loc + t ‚Ä¢ d) ‚àà Metric.ball loc rad := by
    intro t ht; rcases ht with ‚ü®ht1, ht2‚ü©
    rw [Metric.mem_ball, dist_eq_norm, ‚Üê add_sub_cancel' loc (t ‚Ä¢ d)]
    rw [add_sub_cancel', add_sub_cancel', norm_smul, t.norm_eq_abs, abs_of_nonneg ht1]
    apply lt_of_lt_of_le'
    ¬∑ show rad / 2 < rad
      apply half_lt_self
      simp [hs, he]
    ¬∑ calc _ ‚â§ rad1 * ‚Äñd‚Äñ := (mul_le_mul_right (norm_pos_iff.mpr t2)).mpr ht2
           _ = rad * (‚Äñd‚Äñ * ‚Äñd‚Äñ‚Åª¬π) / 2 := by field_simp; ring_nf
           _ = rad / 2 := by rw [mul_inv_cancel t1, mul_one]
  have hgg : ‚àÉ T ‚àà Ioc 0 rad1, ‚àÄ t ‚àà Icc 0 T, g t < 0 := by
    have hcon : ContinuousAt g 0 := by
      simp
      apply ContinuousAt.neg
      apply ContinuousAt.inner continuousAt_const
      have : (fun u => gradient p.objective u) ‚àò (fun u => loc + - (u ‚Ä¢ gradient p.objective loc))
          = (fun (t : ‚Ñù) => gradient p.objective (loc + - (t ‚Ä¢ gradient p.objective loc))) := by
        ext; simp
      have t1 : ContinuousAt (fun u => gradient p.objective u)
          (loc + - ((0 : ‚Ñù) ‚Ä¢ gradient p.objective loc)) := by
        rw [zero_smul, ‚Üê sub_eq_add_neg, sub_zero]
        let g : E ‚Üí E := fun u => gradient p.objective u
        have : ContinuousAt g loc := by
          exact gradient_continuous_of_contdiff p.objective pos hd
        exact this
      have t2 : ContinuousAt (fun (u : ‚Ñù) => loc + - (u ‚Ä¢ gradient p.objective loc)) 0 := by
        apply continuousAt_const.add
        apply (ContinuousAt.smul continuous_id'.continuousAt continuousAt_const).neg
      rw [‚Üê this]
      exact ContinuousAt.comp t1 t2
    have hcon' : ‚àÄ Œµ > 0, ‚àÉ Œ¥ > 0, ‚àÄ (y : ‚Ñù), ‚Äñy - 0‚Äñ < Œ¥ ‚Üí ‚Äñg y - g 0‚Äñ < Œµ := continuous hcon
    rcases hcon' (- (g 0) / 2) (by linarith) with ‚ü®Œ¥, ‚ü®hŒ¥1, hŒ¥2‚ü©‚ü©
    use min (Œ¥ / 2) rad1
    constructor
    ¬∑ constructor
      ¬∑ rw [lt_min_iff]; constructor <;> linarith
      ¬∑ apply min_le_right
    ¬∑ intros t ht
      have th1 : ‚Äñt - 0‚Äñ < Œ¥ := by
        rw [Real.norm_eq_abs, sub_zero, abs_lt]
        rcases ht with ‚ü®ht1, ht2‚ü©; rw [le_min_iff] at ht2
        exact ‚ü®by linarith [ht1], by linarith [(ht2).1]‚ü©
      have th2 : ‚Äñg t - g 0‚Äñ < - (g 0) / 2 := hŒ¥2 t th1
      have : g t - g 0 < - (g 0) / 2 := by
        rw [Real.norm_eq_abs, abs_lt] at th2; exact th2.2
      have : g t < g 0 / 2 := by linarith
      apply lt_trans this (by linarith)
  rcases hgg with ‚ü®T, ‚ü®‚ü®hT1, hT2‚ü©, hg'‚ü©‚ü©
  have hcc : ‚àÉ (a : ‚Ñù), a ‚àà Ioc 0 T ‚àß loc + a ‚Ä¢ d ‚àà v := by
    use T / 2;
    constructor
    ¬∑ constructor <;> linarith
    ¬∑ have : loc + (T / 2) ‚Ä¢ d ‚àà Metric.ball loc rad := by
        apply hin; constructor <;> linarith
      exact hy this
  rcases hcc with ‚ü®a, ‚ü®‚ü®ha1, ha2‚ü©, hcc‚ü©‚ü©
  have : p.objective (loc + a ‚Ä¢ d) < p.objective loc := by
    let p' := fun t ‚Ü¶ gradient p.objective t
    have : ‚àÄ y ‚àà Metric.closedBall loc ‚Äña ‚Ä¢ d‚Äñ, HasGradientAt p.objective (p' y) y := by
      intro y hy
      have hh : y ‚àà Metric.ball loc rad := by
        rw [Metric.mem_ball];
        rw [Metric.mem_closedBall, norm_smul, a.norm_eq_abs, abs_of_pos ha1] at hy;
        have t3 : ‚Äñgradient p.objective loc‚Äñ ‚â† 0 := by simp [hn]
        calc dist y loc ‚â§ rad1 * ‚Äñd‚Äñ := by
                apply le_trans hy
                rw [mul_le_mul_right (norm_pos_iff.mpr t2)]
                exact le_trans ha2 hT2
             _ = rad / 2 := by field_simp; ring_nf;
             _ < rad := by linarith
      have : DifferentiableAt ‚Ñù p.objective y := by
        apply DifferentiableOn.differentiableAt (hd.differentiableOn (by norm_num))
        rw [mem_nhds_iff]; use Metric.ball loc rad;
        exact ‚ü®Eq.subset rfl, ‚ü®Metric.isOpen_ball, hh‚ü©‚ü©
      exact this.hasGradientAt
    have hh : ‚àÉ t : ‚Ñù, t > 0 ‚àß t < 1 ‚àß p.objective (loc + a ‚Ä¢ d) =
        p.objective loc + inner (p' (loc + t ‚Ä¢ (a ‚Ä¢ d))) (a ‚Ä¢ d) := by
      exact general_expansion _ _ this
    rcases hh with ‚ü®t, ‚ü®ht1, ht2, hh‚ü©‚ü©
    have hff : inner (p' (loc + t ‚Ä¢ a ‚Ä¢ d)) (a ‚Ä¢ d) < (0 : ‚Ñù) := by
      have : inner (p' (loc + t ‚Ä¢ a ‚Ä¢ d)) d = g (t ‚Ä¢ a) := by
        dsimp [g]; rw [smul_smul, real_inner_comm]
      rw [inner_smul_right, this, mul_comm]
      apply mul_neg_of_neg_of_pos _ ha1
      apply hg' (t ‚Ä¢ a)
      constructor <;> simp [ha1, ha2, ht1, ht2]
      linarith
      rw [‚Üê one_mul T]; apply mul_le_mul (by linarith) (by linarith) (by linarith) (by norm_num)
    rw [hh, add_lt_iff_neg_left]; exact hff
  have : p.objective (loc + a ‚Ä¢ d) ‚â• p.objective loc := by exact hv2 (loc + a ‚Ä¢ d) hcc
  linarith

/-
When f is convex, any local minimizer x‚àó is a global minimizer of f.
Numerical Optimization, Nocedal & Wright, Theorem 2.5
-/
theorem convex_local_minima_is_global_minima (h : ConvexOn ‚Ñù univ p.objective)
    (hf : ContDiff ‚Ñù 1 p.objective) (hl : p.Local_Minima loc):
    p.Global_Minima loc := by
  have : gradient p.objective loc = (0 : E) := by
    apply first_order_unconstrained_necessary p loc _ hl
    exact ‚ü®1, by norm_num, ContDiff.contDiffOn hf‚ü©
  have hg : HasGradientAt p.objective ((fun t ‚Ü¶ gradient p.objective t) loc) loc :=
    (hf.differentiable (by norm_num) _).hasGradientAt
  have mini : ‚àÄ y, p.objective loc ‚â§ p.objective y := by
    intro y
    convert first_order_condition' hg h (by trivial) (by trivial)
    rw [this]; simp
  exact isMinOn_univ_iff.mpr mini

theorem convex_local_minima_is_global_minima' (h : ConvexOn ‚Ñù univ p.objective)
    (hf : ‚àÉ Œµ > 0, ContDiffOn ‚Ñù 1 p.objective (Metric.ball loc Œµ)) (hl : p.Local_Minima loc) :
    p.Global_Minima loc := by
  have : gradient p.objective loc = (0 : E) := by
    apply first_order_unconstrained_necessary p loc hf hl
  have hg : HasGradientAt p.objective ((fun t ‚Ü¶ gradient p.objective t) loc) loc := by
    rcases hf with ‚ü®e, ‚ü®he, hdf‚ü©‚ü©
    have hdf' : DifferentiableAt ‚Ñù p.objective loc := by
      apply DifferentiableOn.differentiableAt (hdf.differentiableOn (by norm_num))
      exact Metric.ball_mem_nhds loc he
    exact hdf'.hasGradientAt
  have mini : ‚àÄ y, p.objective loc ‚â§ p.objective y := by
    intro y
    convert first_order_condition' hg h (by trivial) (by trivial)
    rw [this]; simp
  exact isMinOn_univ_iff.mpr mini

/-
When f is convex and differentiable, then any stationary point x‚àó is a global minimizer of f.
Numerical Optimization, Nocedal & Wright, Theorem 2.5
-/
theorem convex_stationary_point_is_global_minima
    (h : ConvexOn ‚Ñù univ p.objective) (hs : p.Stationary_Point loc) :
    p.Global_Minima loc := by
  rcases hs with ‚ü®hdf, hg0‚ü©
  have mini : ‚àÄ y, p.objective loc ‚â§ p.objective y := by
    intro y
    convert first_order_condition' (hdf loc).hasGradientAt h (by trivial) (by trivial)
    rw [hg0]; simp
  exact isMinOn_univ_iff.mpr mini

end First_Order_Optimality

end optimization
